\documentclass[a4paper,10pt,notitlepage]{article}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{url}
\usepackage{hyperref}
\usepackage{listings}
\usepackage[usenames,dvipsnames]{color}
\usepackage[stable]{footmisc}
\usepackage{caption}
\usepackage{hyphenat}
\usepackage{courier}
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{\colorbox{Gray}{\parbox{\textwidth}{#1#2#3}}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}
\definecolor{lightgray}{gray}{0.9}

\lstset{
	language=C,
	basicstyle=\small\ttfamily,
	commentstyle=\color{Gray},
	tabsize=4,
	numbers=left,
	numberstyle=\tiny,
	stepnumber=1,
	numbersep=5pt,
	backgroundcolor=\color{lightgray},
	}

\title{Parallel Algorithms and Parallel Computers (IN4026) \\ Lab report}
\author{Źmicier Žaleźničenka (4134575) \\ \\
D.V.Zhaleznichenka@student.tudelft.nl}
\date{\today}

\begin{document}

\maketitle

\section{Prefix/suffix minima problem}

\subsection{Introduction}

In the first assignment it was required to devise an efficient algorithm computing prefix and suffix minimas for the given array\footnote{Given an array \(A = (a_1, . . . , a_N)\) with elements drawn from a linear ordered set. The suffix minima problem is to determine \(min\{a_i, a_{i+1}, . . . a_N\}\) for each $i$. The prefix minima are \(min\{a_1, a_2, . . . a_i\}\) for each $i$.}. 

Despite it is easy to compute prefix/suffix minimas sequentially with \(T = O(N)\), it was required to find an algorithm that would have better performance in the parallel setup as the performance of the sequential solution for the large data sets is not satisfactory.

The algorithm idea was found at Wikipedia page explaining prefix sums\footnote{$http://en.wikipedia.org/wiki/Prefix\_sum$}. The algorithm is recursive and consists of three steps. First of all, we compute the minimas of consecutive pairs of items in which the first item of the pair has an even index and store them in a separate array. Secondly, we recursively compute the minimas for the obtained array. At the last step, we expand the computed sequence with the help of the source array. Here, each element is either directly copied from the previously computed sequence or is calculated using the element in the source array and a partial minima from the previously computed sequence. The first and the third steps in this algorithm can be parallelized easily. The second step of the algorithm is the recursive call that eventually expands into a series of the first and the third steps, thus not being a bottleneck of the algorithm. 

\subsection{Algorithm proof}

The recursion is performed in two steps. In the first step for several times we discard from the original sequence half of its elements. We do it in such a way that all the remaining elements are the least in the consequent element pairs of the source seqence. We reduce the sequence until only one element remains in it. By design, this element is the minima of the sequence. 

After we find the minimal element, we expand the sequence back. At each recursion step up to the original sequence for each element in the source sequence we get a local minima, i.e. the minima of preceeding sequence (for prefix minima problem) or the minima of the subsequent sequence (for suffix minima problem). To find out the element that should be placed in a prefix/suffix minima sequence instead of the original element, we compare the original element with the local minima and select the smallest element for odd values or just take the local minima for the even values, as the local minima already concerns the original element in this case. 

\subsection{Time-complexity analysis}

The time-work calculations here were done for the prefix minima problem. The computational complexity of the suffix minima problem is the same.

\begin{lstlisting}

//prefix(A, N) - find prefix minima for array A of size N
//input:  array A[0..N-1]
//output: array B[0..N-1] array of prefix minimas for A

1.  for 0 < i < N pardo //W = O(N), T = O(1)
		if i is even then Z[i/2] = min(A[i],A[i+1])

2.  Z = prefix(Z, N/2)		//W = W(N/2), T = T(N/2)

3.  for 1 < i < N pardo //W = O(N), T = O(1)
		if i is odd then B[i] = Z[i/2]
		else B[i] = min(A[i], Z[i/2-1])

//T(N) = T(N/2) + O(1) = O(log n)
//W(N) = W(N/2) + O(N) = O(N)		
	
\end{lstlisting}

\subsection{Implementation}

The algorithm reads source data from the file. The output of the results is suppressed but it can be enabled with the help of the preprocessor directive. The algorithm was tested with the sequence suggested in the task description (file base.test), increasing sequence (inc.test) and decreasing sequence (dec.test). To do the performance tests described in the following section, a generated test file bigdata.test containing an array with 262144 elements was used. The data generator is written in Python and can be found in datagen.py file.

The sequential implementation of the algorithm is straightforward and is based on pseudocode provided in the previous section. The function \lstinline{scan_seq()} accepts the source array, its length and prefix value as the parameters. The prefix value is needed to distinguish between prefix and suffix minima computations. 

The sequential implementation contains three OpenMP directives needed to parallelize the work-intensive parts of the algorithm. To parallelize the first step of the algorithm we need in one pragma and to parallelilize the third step we use two pragmas, one for prefix computations and one for suffix computations. All the pragmas have the following structure.

\begin{lstlisting}
#pragma omp parallel for \
	shared (Z, source, chunk, num_threads) private (i) \
	schedule (static, chunk) \
	num_threads (num_threads)
\end{lstlisting}

In each round of computations, \lstinline{scan_seq()} function is invoked twice, once for prefix computations and once for suffix computations. Though it is possible to compute both prefixes and suffixes in one round we have decided to split the computations as according to the requirements both resulting arrays should be stored in array $B$. Thus, we have to reinitialize array $B$ between the computations. This is done in the function \lstinline{seq_function()}. While doing performance tests, we measure the execution time of this function.

The threaded version of the algorithm is organized the same way. We have a function \lstinline{par_function()} that calls \lstinline{scan_par()} function twice and reinitializes $B$ in between. All the computations are performed in \lstinline{scan_par()} function. 

We have rewritten the code of \lstinline{main()} function to move the pthreads-related code from it to \lstinline{scan_par()} function. We think that our implementation of the threaded version required such a change. 

As we did it with the OpenMP part, we have parallelized the first and the third steps of the algorithm. The first step is implemented in function \lstinline{par_sum()}, the third is implemented in function \lstinline{par_odd}. At each recursion step, we create the required number of threads in the main thread executing the code of \lstinline{scan_par()} function, wait for the completion of the first step, destroy the threads and go deeper. When the recursion is over and we get back, we create the new threads, wait for the completion of the third step and destroy the threads. Here, we stick to the boss-worker threading model.

With this solution, we have an additional overhead due to the need to create and destroy the worker threads in each recursion step. It would make more sense to create the threads pool only once and feed them with the new parameters when needed. However, we were not able to implement the thread pool correctly with pthreads framework.

We have tried to optimize the creation of threads by limiting their number for non-intensive operations. At the last steps of recursion, when there are only several values to compute, it is cheaper not to create many threads for this task, but to assign all the tasks to one thread only, or to the small number of threads. The logic of this improvement is implemented in \lstinline{get_optimal_threads_number()} function. However, it turned out that this optimization didn't lead to the significant improvement as the largest part of the work is done when all the threads are processing big tasks, and the final recursion steps can be computed rather quickly using any number of threads. But this optimization was helpful anyway as we needed to limit the number of threads for the situations with less values than threads.   

In our solution, at each recursion step, the problem space is divided into subspaces that are assigned to the worker threads. Each thread computes a fixed amount of $N/nrT$ values. The subspaces do not overlap, thus it is safe for the threads to read and write needed values concurrently.

\subsection{Tests}

\subsection{Conclusions}


\section{Simple merge problem}

\subsection{Introduction}

\subsection{Algorithm proof}

\subsection{Time-complexity analysis}

\subsection{Implementation}

\subsection{Tests}

\subsection{Conclusions}


\section{List ranking (pointer jumping) problem}

\subsection{Introduction}

\subsection{Algorithm proof}

\subsection{Time-complexity analysis}

\subsection{Implementation}

\subsection{Tests}

\subsection{Conclusions}

\end{document}
